{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Pyclustering dependencies\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from pyclustering.utils.metric import distance_metric, type_metric\n",
    "\n",
    "# Global seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Folder structure configurations\n",
    "RESULTS_DIR = \"results_17_01\"   # JSON input\n",
    "RESULTS_CSV_DIR = \"results_csv_clean_2\" # Output root for CSVs and clustering\n",
    "os.makedirs(RESULTS_CSV_DIR, exist_ok=True)\n",
    "\n",
    "# Known folder “labels”\n",
    "S_FOLDERS   = [\"s_8\", \"s_15\", \"s_25\"]\n",
    "DISTANCES   = [\"smooth\", \"uniform\", \"hamming\", \"overall\"]\n",
    "NOISE_LEVELS = [\"n_0.02\", \"n_0.04\", \"n_0.06\"]\n",
    "\n",
    "# Map s_x to # of clusters\n",
    "CLUSTER_MAPPING = {\n",
    "    \"s_8\": 8,\n",
    "    \"s_15\": 15,\n",
    "    \"s_25\": 25\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename, s_values, distances, noise_values):\n",
    "    \"\"\"\n",
    "    Parse the filename to find:\n",
    "      - which s_folder (e.g., 's_8', 's_15', 's_25')\n",
    "      - which distance ('smooth', 'uniform', 'hamming', 'overall')\n",
    "      - which noise level ('n_0.02', 'n_0.04', 'n_0.06')\n",
    "\n",
    "    Returns: (s_folder, distance, noise)\n",
    "\n",
    "    Raises ValueError if anything is missing or not found.\n",
    "    \"\"\"\n",
    "    # 1. Find s_folder\n",
    "    s_folder = None\n",
    "    for s_val in s_values:\n",
    "        if s_val in filename:\n",
    "            s_folder = s_val\n",
    "            break\n",
    "    if not s_folder:\n",
    "        raise ValueError(f\"Could not parse s_folder (s_8, s_15, s_25) from {filename}\")\n",
    "\n",
    "    # 2. Find distance\n",
    "    distance = None\n",
    "    for dist in distances:\n",
    "        if dist in filename:\n",
    "            distance = dist\n",
    "            break\n",
    "    # If nothing found, default to \"smooth\"\n",
    "    if not distance:\n",
    "        distance = \"smooth\"\n",
    "\n",
    "    # 3. Find noise level\n",
    "    noise = None\n",
    "    for nv in noise_values:\n",
    "        if nv in filename:\n",
    "            noise = nv\n",
    "            break\n",
    "    if not noise:\n",
    "        raise ValueError(f\"Could not parse noise level (n_0.02, n_0.04, n_0.06) from {filename}\")\n",
    "\n",
    "    return s_folder, distance, noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path):\n",
    "    \"\"\"\n",
    "    Loads JSON data into a dictionary of DataFrames/Series\n",
    "    (similar to your existing logic).\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    parsed_data = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, list):\n",
    "            try:\n",
    "                arr = np.array(value)\n",
    "                if arr.ndim == 1:\n",
    "                    parsed_data[key] = pd.Series(value)\n",
    "                elif arr.ndim == 2:\n",
    "                    parsed_data[key] = pd.DataFrame(value)\n",
    "                else:\n",
    "                    # If it's a list of lists-of-lists, convert each slice\n",
    "                    parsed_data[key] = {\n",
    "                        i: pd.DataFrame(subarr) for i, subarr in enumerate(arr)\n",
    "                    }\n",
    "            except ValueError:\n",
    "                # Fallback if direct array conversion fails\n",
    "                parsed_data[key] = {\n",
    "                    i: pd.DataFrame(subarr) for i, subarr in enumerate(value) if isinstance(subarr, list)\n",
    "                }\n",
    "        else:\n",
    "            parsed_data[key] = value\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "def create_folder_structure():\n",
    "    \"\"\"\n",
    "    Create the 3-level folder structure under RESULTS_CSV_DIR:\n",
    "        s_* / distance / n_*\n",
    "    But do not create subfolders yet for Dw/Dkl; that happens dynamically.\n",
    "    \"\"\"\n",
    "    for s_val in S_FOLDERS:\n",
    "        s_path = os.path.join(RESULTS_CSV_DIR, s_val)\n",
    "        os.makedirs(s_path, exist_ok=True)\n",
    "        for dist in DISTANCES:\n",
    "            dist_path = os.path.join(s_path, dist)\n",
    "            os.makedirs(dist_path, exist_ok=True)\n",
    "            for nv in NOISE_LEVELS:\n",
    "                noise_path = os.path.join(dist_path, nv)\n",
    "                os.makedirs(noise_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_json_files():\n",
    "    \"\"\"\n",
    "    For each JSON in RESULTS_DIR:\n",
    "      1) parse s_folder, distance, noise\n",
    "      2) load JSON data\n",
    "      3) create subfolder named after the filename (minus .json)\n",
    "      4) for each key in 'parsed_data' that starts with 'all_', create a\n",
    "         corresponding sub-subfolder and save each matrix to CSV.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(RESULTS_DIR):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "        \n",
    "        # parse s_folder, distance, noise\n",
    "        s_folder, distance, noise = parse_filename(\n",
    "            filename,\n",
    "            s_values=S_FOLDERS,\n",
    "            distances=DISTANCES,\n",
    "            noise_values=NOISE_LEVELS\n",
    "        )\n",
    "        \n",
    "        # load data\n",
    "        json_path = os.path.join(RESULTS_DIR, filename)\n",
    "        parsed_data = load_data(json_path)\n",
    "\n",
    "        # build the subfolder path: results_csv/s_folder/distance/noise/<run_name>/\n",
    "        run_name = os.path.splitext(filename)[0]\n",
    "        run_path = os.path.join(RESULTS_CSV_DIR, s_folder, distance, noise, run_name)\n",
    "        os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "        # For each key in parsed_data that starts with \"all_\", create a subfolder\n",
    "        for key, val in parsed_data.items():\n",
    "            if not key.startswith(\"all_\"):\n",
    "                continue\n",
    "\n",
    "            # e.g., key = \"all_Dw\" => subfolder name = \"Dw\"\n",
    "            subfolder_name = key.replace(\"all_\", \"\")\n",
    "            subfolder_path = os.path.join(run_path, subfolder_name)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "            # val could be a dict of DataFrames or a single DataFrame\n",
    "            # Often in your code it's a dict: { i: DataFrame, i: DataFrame, ... }\n",
    "            # So handle that scenario:\n",
    "            if isinstance(val, dict):\n",
    "                # Save each matrix to CSV\n",
    "                for i, df in val.items():\n",
    "                    csv_path = os.path.join(subfolder_path, f\"{subfolder_name}_{i}.csv\")\n",
    "                    df.to_csv(csv_path, index=False, header=False)\n",
    "            elif isinstance(val, pd.DataFrame):\n",
    "                # Single DF scenario\n",
    "                csv_path = os.path.join(subfolder_path, f\"{subfolder_name}.csv\")\n",
    "                val.to_csv(csv_path, index=False, header=False)\n",
    "            else:\n",
    "                # Possibly a Series or something else\n",
    "                # Adjust if needed\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pam_cosine_clustering(data, n_clusters, random_seed=SEED):\n",
    "    \"\"\"\n",
    "    Performs k-medoids (PAM) with cosine distance.\n",
    "    data shape expected: (num_bins, num_signatures).\n",
    "    Returns: (clusters, medoid_signatures)\n",
    "    \"\"\"\n",
    "    # Convert to NumPy\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.to_numpy()\n",
    "\n",
    "    # Transpose => (num_signatures, num_bins)\n",
    "    data = data.T\n",
    "    num_signatures, _ = data.shape\n",
    "\n",
    "    if n_clusters > num_signatures:\n",
    "        raise ValueError(f\"Requested {n_clusters} clusters, but only {num_signatures} signatures available.\")\n",
    "\n",
    "    # Define custom distance\n",
    "    def custom_cosine_distance(a, b):\n",
    "        return cdist([a], [b], \"cosine\")[0, 0]\n",
    "\n",
    "    metric = distance_metric(type_metric.USER_DEFINED, func=custom_cosine_distance)\n",
    "    initial_medoids = random.sample(range(num_signatures), n_clusters)\n",
    "\n",
    "    kmed = kmedoids(data.tolist(), initial_medoids, metric=metric)\n",
    "    kmed.process()\n",
    "    clusters = kmed.get_clusters()\n",
    "    medoids = kmed.get_medoids()\n",
    "    medoid_signatures = np.array([data[idx] for idx in medoids])\n",
    "    return clusters, medoid_signatures\n",
    "\n",
    "def load_and_cluster(folder_path, n_clusters, output_csv_path, random_seed=SEED):\n",
    "    \"\"\"\n",
    "    Loads CSV files in `folder_path`, concatenates them, does PAM clustering,\n",
    "    and saves the medoids to `output_csv_path`.\n",
    "    \"\"\"\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files in {folder_path}, skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    matrices = []\n",
    "    for f in csv_files:\n",
    "        mat = pd.read_csv(os.path.join(folder_path, f), header=None)\n",
    "        matrices.append(mat)\n",
    "\n",
    "    combined_data = pd.concat(matrices, axis=1)  # horizontally\n",
    "    clusters, medoids = pam_cosine_clustering(combined_data, n_clusters, random_seed)\n",
    "\n",
    "    # Save medoids\n",
    "    pd.DataFrame(medoids.T).to_csv(output_csv_path, index=False, header=False)\n",
    "    print(f\"Medoid signatures saved to: {output_csv_path}\")\n",
    "    \n",
    "    return clusters, medoids\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_csv /\n",
    "    s_15 /\n",
    "        uniform /\n",
    "            n_0.02 /\n",
    "                <run_name> /\n",
    "                    Dw /\n",
    "                    Dkl /\n",
    "                    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_folders(root_dir, s_values, distances, noise_values):\n",
    "    \"\"\"\n",
    "    Yields (s_folder, distance, noise, run_folder, subfolder_path)\n",
    "    for each folder that actually exists under:\n",
    "        root_dir / s_folder / distance / noise / run_folder / ...\n",
    "    We'll pick up any sub-subfolders under run_folder.\n",
    "    \"\"\"\n",
    "    for s_val in s_values:\n",
    "        s_path = os.path.join(root_dir, s_val)\n",
    "        if not os.path.exists(s_path):\n",
    "            continue\n",
    "        for dist in distances:\n",
    "            dist_path = os.path.join(s_path, dist)\n",
    "            if not os.path.exists(dist_path):\n",
    "                continue\n",
    "            for nv in noise_values:\n",
    "                noise_path = os.path.join(dist_path, nv)\n",
    "                if not os.path.exists(noise_path):\n",
    "                    continue\n",
    "                # each run_folder is the JSON-based folder name\n",
    "                for run_folder in os.listdir(noise_path):\n",
    "                    run_path = os.path.join(noise_path, run_folder)\n",
    "                    if not os.path.isdir(run_path):\n",
    "                        continue\n",
    "                    # now yield any subfolder inside run_path\n",
    "                    for subfolder in os.listdir(run_path):\n",
    "                        subfolder_path = os.path.join(run_path, subfolder)\n",
    "                        if os.path.isdir(subfolder_path):\n",
    "                            yield (s_val, dist, nv, run_folder, subfolder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering():\n",
    "    for s_val, dist, noise, run_name, subfolder_path in traverse_folders(\n",
    "        root_dir=RESULTS_CSV_DIR,\n",
    "        s_values=S_FOLDERS,\n",
    "        distances=DISTANCES,\n",
    "        noise_values=NOISE_LEVELS\n",
    "    ):\n",
    "        # Filter so we only process \"Dw\" or \"Dkl\" subfolders\n",
    "        subfolder_name = os.path.basename(subfolder_path)\n",
    "        if subfolder_name not in (\"Dw\", \"Dkl\"):\n",
    "            continue\n",
    "\n",
    "        n_clusters = CLUSTER_MAPPING[s_val]\n",
    "        output_csv = os.path.join(subfolder_path, f\"medoids_{subfolder_name}.csv\")\n",
    "\n",
    "        print(f\"Clustering {subfolder_path} -> {output_csv} with k={n_clusters}\")\n",
    "        clusters, medoids = load_and_cluster(\n",
    "            folder_path=subfolder_path,\n",
    "            n_clusters=n_clusters,\n",
    "            output_csv_path=output_csv,\n",
    "            random_seed=SEED\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Suppose these are the known JSON filenames for each s_x:\n",
    "file_map = {\n",
    "    \"s_8\":  \"Results_20250117_154939_overall_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54.json\",\n",
    "    \"s_15\": \"Results_20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93.json\",\n",
    "    \"s_25\": \"Results_20250117_154536__s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46\"\n",
    "}\n",
    "\n",
    "def save_ground_truth_signatures(cosmic_signatures_path):\n",
    "    \"\"\"\n",
    "    Loads the cosmic signatures, extracts the relevant SBS IDs from each JSON\n",
    "    file name (the portion after GRCh37_), and saves them to CSV in their\n",
    "    corresponding s_x folder.\n",
    "    \"\"\"\n",
    "    cosmic_df = pd.read_csv(cosmic_signatures_path, sep=\"\\t\")\n",
    "    # If there's a column \"Type\", set as index\n",
    "    if \"Type\" in cosmic_df.columns:\n",
    "        cosmic_df.set_index(\"Type\", inplace=True)\n",
    "\n",
    "    # For each s_folder + known JSON filename:\n",
    "    for s_val, fname in file_map.items():\n",
    "        match = re.search(r\"GRCh37_([\\w\\d_]+)\", fname)\n",
    "        if not match:\n",
    "            print(f\"No signature portion found in {fname}, skipping.\")\n",
    "            continue\n",
    "        sig_portion = match.group(1)\n",
    "        sbs_ids = sig_portion.split(\"_\")  # like [\"98\",\"86\",\"39\",\"22a\",...]\n",
    "        # Prepend \"SBS\" to each id\n",
    "        # watch out if you have \"22a\" or \"17b\" etc. — adjust if needed\n",
    "        signature_names = [f\"SBS{id_}\" for id_ in sbs_ids]\n",
    "\n",
    "        # Filter cosmic\n",
    "        filtered_sigs = cosmic_df[signature_names]\n",
    "\n",
    "        # Save to CSV\n",
    "        out_path = os.path.join(RESULTS_CSV_DIR, s_val, f\"ground_truth_signatures_{s_val}.csv\")\n",
    "        filtered_sigs.to_csv(out_path, index=True)\n",
    "        print(f\"Saved ground truth to {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create the standard 3-level directory \n",
    "create_folder_structure()\n",
    "\n",
    "# 2) Convert JSON => CSV subfolders\n",
    "process_json_files()\n",
    "\n",
    "# 3) Run clustering on each subfolder\n",
    "run_clustering()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation(true_sigs, est_sigs, cutoff=0.9, dist=\"cos\"):\n",
    "    \"\"\"\n",
    "    Evaluate extracted sigs (est_sigs) vs. true_sigs.\n",
    "    Returns a tuple with (n_ground_truth, n_detected, tp, fp, fn, precision, recall, f1, index_map).\n",
    "    \"\"\"\n",
    "    if true_sigs.shape[1] >= est_sigs.shape[1]:\n",
    "        mat1, mat2 = est_sigs, true_sigs\n",
    "    else:\n",
    "        mat1, mat2 = true_sigs, est_sigs\n",
    "\n",
    "    if dist == \"cos\":\n",
    "        con_mat = cdist(mat1.T, mat2.T, \"cosine\")\n",
    "    elif dist == \"cor\":\n",
    "        con_mat = cdist(mat1.T, mat2.T, \"correlation\")\n",
    "    else:\n",
    "        raise ValueError(\"Distance must be 'cos' or 'cor'\")\n",
    "\n",
    "    # Hungarian assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(con_mat)\n",
    "    similarity_mat = 1 - con_mat  # convert distance -> similarity\n",
    "\n",
    "    idx_map = {}\n",
    "    tp = 0\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        idx_map[r] = c\n",
    "        if similarity_mat[r, c] >= cutoff:\n",
    "            tp += 1\n",
    "\n",
    "    computed_fp = mat1.shape[1] - tp\n",
    "    computed_fn = computed_fp\n",
    "    if true_sigs.shape[1] >= est_sigs.shape[1]:\n",
    "        base_fp = 0\n",
    "        base_fn = true_sigs.shape[1] - est_sigs.shape[1]\n",
    "    else:\n",
    "        base_fp = est_sigs.shape[1] - true_sigs.shape[1]\n",
    "        base_fn = 0\n",
    "\n",
    "    fp = base_fp + computed_fp\n",
    "    fn = base_fn + computed_fn\n",
    "\n",
    "    n_gt = true_sigs.shape[1]\n",
    "    n_detected = est_sigs.shape[1]\n",
    "\n",
    "    try:\n",
    "        precision = round(tp / (tp + fp), 2)\n",
    "        recall    = round(tp / (tp + fn), 2)\n",
    "        f1_score  = round(2 * precision * recall / (precision + recall), 2)\n",
    "    except ZeroDivisionError:\n",
    "        precision, recall, f1_score = 0, 0, 0\n",
    "\n",
    "    return (n_gt, n_detected, tp, fp, fn, precision, recall, f1_score, idx_map)\n",
    "\n",
    "def run_evaluation(cutoff=0.9):\n",
    "    \"\"\"\n",
    "    For each subfolder with medoids_<subfolder>.csv,\n",
    "    read the corresponding ground truth, evaluate, save results in .txt,\n",
    "    but ONLY for 'Dw' or 'Dkl'.\n",
    "    \"\"\"\n",
    "    for s_val, dist, noise, run_name, subfolder_path in traverse_folders(\n",
    "        root_dir=RESULTS_CSV_DIR,\n",
    "        s_values=S_FOLDERS,\n",
    "        distances=DISTANCES,\n",
    "        noise_values=NOISE_LEVELS\n",
    "    ):\n",
    "        subfolder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        # Skip any subfolder that is not Dw or Dkl\n",
    "        if subfolder_name not in (\"Dw\", \"Dkl\"):\n",
    "            continue\n",
    "        \n",
    "        gt_csv = os.path.join(RESULTS_CSV_DIR, s_val, f\"ground_truth_signatures_{s_val}.csv\")\n",
    "        if not os.path.exists(gt_csv):\n",
    "            continue\n",
    "        \n",
    "        true_signatures = pd.read_csv(gt_csv, index_col=0).to_numpy()\n",
    "        \n",
    "        # Medoids file path depends on subfolder name (Dw or Dkl)\n",
    "        medoids_csv = os.path.join(subfolder_path, f\"medoids_{subfolder_name}.csv\")\n",
    "        if not os.path.exists(medoids_csv):\n",
    "            continue\n",
    "        \n",
    "        est_signatures = pd.read_csv(medoids_csv, header=None).to_numpy()\n",
    "        \n",
    "        # Run evaluation\n",
    "        eval_res = evaluation(\n",
    "            true_sigs=true_signatures,\n",
    "            est_sigs=est_signatures,\n",
    "            cutoff=cutoff,\n",
    "            dist=\"cos\"\n",
    "        )\n",
    "        \n",
    "        # Write results\n",
    "        eval_txt = os.path.join(subfolder_path, f\"evaluation_{subfolder_name}.txt\")\n",
    "        with open(eval_txt, \"w\") as f:\n",
    "            f.write(\"=== Evaluation Results ===\\n\")\n",
    "            f.write(f\"Cutoff: {cutoff}\\n\")\n",
    "            f.write(f\"s_folder: {s_val}\\n\")\n",
    "            f.write(f\"distance: {dist}\\n\")\n",
    "            f.write(f\"noise: {noise}\\n\")\n",
    "            f.write(f\"run_folder: {run_name}\\n\")\n",
    "            f.write(f\"subfolder: {subfolder_name}\\n\")\n",
    "            f.write(f\"Number of Ground Truth: {eval_res[0]}\\n\")\n",
    "            f.write(f\"Number of Detected: {eval_res[1]}\\n\")\n",
    "            f.write(f\"True Positives: {eval_res[2]}\\n\")\n",
    "            f.write(f\"False Positives: {eval_res[3]}\\n\")\n",
    "            f.write(f\"False Negatives: {eval_res[4]}\\n\")\n",
    "            f.write(f\"Precision: {eval_res[5]}\\n\")\n",
    "            f.write(f\"Recall: {eval_res[6]}\\n\")\n",
    "            f.write(f\"F1 Score: {eval_res[7]}\\n\")\n",
    "            f.write(\"Index Pairs (Extracted -> Ground Truth):\\n\")\n",
    "            for e_idx, t_idx in eval_res[8].items():\n",
    "                f.write(f\"  Extracted {e_idx} -> Ground Truth {t_idx}\\n\")\n",
    "\n",
    "        print(f\"Evaluation written to {eval_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# EVALUATION WITHOUT HUNGARIAN RE-ARRANGMENT\n",
    "\n",
    "def evaluation_figS1(true_sigs, est_sigs, cutoff=0.9, dist=\"cos\"):\n",
    "    \"\"\"\n",
    "    Evaluate extracted sigs (est_sigs) vs. true_sigs in a “Figure S1” style:\n",
    "      - Each extracted signature is TP if it matches (>= cutoff) at least one ground-truth signature.\n",
    "      - Otherwise it is FP.\n",
    "      - A ground-truth signature is FN if no extracted signature matches it above the cutoff.\n",
    "    Returns:\n",
    "      n_ground_truth, n_detected, tp, fp, fn, precision, recall, f1_score\n",
    "    \"\"\"\n",
    "    # Number of ground-truth and extracted signatures\n",
    "    n_gt = true_sigs.shape[1]\n",
    "    n_detected = est_sigs.shape[1]\n",
    "\n",
    "    # Compute pairwise distance matrix => then convert to similarity\n",
    "    if dist == \"cos\":\n",
    "        D = cdist(est_sigs.T, true_sigs.T, metric=\"cosine\")  # shape: (n_detected, n_gt)\n",
    "    elif dist == \"cor\":\n",
    "        D = cdist(est_sigs.T, true_sigs.T, metric=\"correlation\")\n",
    "    else:\n",
    "        raise ValueError(\"Distance must be 'cos' or 'cor'\")\n",
    "\n",
    "    similarity = 1 - D  # Convert distance to similarity if using cosine/correlation\n",
    "\n",
    "    # (1) Classify each extracted signature\n",
    "    # max similarity to any ground-truth signature\n",
    "    max_sim_extracted = np.max(similarity, axis=1)  # shape: (n_detected,)\n",
    "    # True positives are those that exceed the cutoff\n",
    "    tp = np.sum(max_sim_extracted >= cutoff)\n",
    "    # False positives are those that do not match any ground-truth signature above cutoff\n",
    "    fp = n_detected - tp\n",
    "\n",
    "    # (2) Classify each ground-truth signature\n",
    "    # max similarity of each ground-truth signature to any extracted signature\n",
    "    max_sim_truth = np.max(similarity, axis=0)  # shape: (n_gt,)\n",
    "    # False negatives are ground-truth signatures not matched by any extracted signature\n",
    "    fn = np.sum(max_sim_truth < cutoff)\n",
    "\n",
    "    # (3) Precision/Recall/F1\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "    if (precision + recall) == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    # Round to 2 decimals for reporting\n",
    "    precision = round(precision, 2)\n",
    "    recall    = round(recall, 2)\n",
    "    f1_score  = round(f1_score, 2)\n",
    "\n",
    "    return (n_gt, n_detected, tp, fp, fn, precision, recall, f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_evaluation_figS1(cutoff=0.8):\n",
    "    \"\"\"\n",
    "    For each subfolder with medoids_<subfolder>.csv,\n",
    "    read the corresponding ground truth, evaluate using the “Figure S1” method,\n",
    "    save results in evaluation_figS1_<subfolder>.txt,\n",
    "    but ONLY for 'Dw' or 'Dkl' subfolders.\n",
    "    \"\"\"\n",
    "    for s_val, dist, noise, run_name, subfolder_path in traverse_folders(\n",
    "        root_dir=RESULTS_CSV_DIR,\n",
    "        s_values=S_FOLDERS,\n",
    "        distances=DISTANCES,\n",
    "        noise_values=NOISE_LEVELS\n",
    "    ):\n",
    "        subfolder_name = os.path.basename(subfolder_path)\n",
    "        \n",
    "        # Skip any subfolder that is not Dw or Dkl\n",
    "        if subfolder_name not in (\"Dw\", \"Dkl\"):\n",
    "            continue\n",
    "        \n",
    "        # Path to ground-truth signatures\n",
    "        gt_csv = os.path.join(RESULTS_CSV_DIR, s_val, f\"ground_truth_signatures_{s_val}.csv\")\n",
    "        if not os.path.exists(gt_csv):\n",
    "            continue\n",
    "        \n",
    "        true_signatures = pd.read_csv(gt_csv, index_col=0).to_numpy()\n",
    "        \n",
    "        # Path to medoids (extracted signatures)\n",
    "        medoids_csv = os.path.join(subfolder_path, f\"medoids_{subfolder_name}.csv\")\n",
    "        if not os.path.exists(medoids_csv):\n",
    "            continue\n",
    "        \n",
    "        est_signatures = pd.read_csv(medoids_csv, header=None).to_numpy()\n",
    "        \n",
    "        # Run “Figure S1” evaluation\n",
    "        eval_res = evaluation_figS1(\n",
    "            true_sigs=true_signatures,\n",
    "            est_sigs=est_signatures,\n",
    "            cutoff=cutoff,\n",
    "            dist=\"cos\"  # or \"cor\" if you want correlation\n",
    "        )\n",
    "        \n",
    "        # Unpack results\n",
    "        n_gt, n_detected, tp, fp, fn, precision, recall, f1_score = eval_res\n",
    "        \n",
    "        # Write results\n",
    "        eval_txt = os.path.join(subfolder_path, f\"evaluation_figS1_{subfolder_name}.txt\")\n",
    "        with open(eval_txt, \"w\") as f:\n",
    "            f.write(\"=== Evaluation Results (Figure S1 Style) ===\\n\")\n",
    "            f.write(f\"Cutoff: {cutoff}\\n\")\n",
    "            f.write(f\"s_folder: {s_val}\\n\")\n",
    "            f.write(f\"distance: {dist}\\n\")\n",
    "            f.write(f\"noise: {noise}\\n\")\n",
    "            f.write(f\"run_folder: {run_name}\\n\")\n",
    "            f.write(f\"subfolder: {subfolder_name}\\n\")\n",
    "            f.write(f\"Number of Ground Truth: {n_gt}\\n\")\n",
    "            f.write(f\"Number of Detected: {n_detected}\\n\")\n",
    "            f.write(f\"True Positives: {tp}\\n\")\n",
    "            f.write(f\"False Positives: {fp}\\n\")\n",
    "            f.write(f\"False Negatives: {fn}\\n\")\n",
    "            f.write(f\"Precision: {precision}\\n\")\n",
    "            f.write(f\"Recall: {recall}\\n\")\n",
    "            f.write(f\"F1 Score: {f1_score}\\n\")\n",
    "\n",
    "        print(f\"Figure S1-style evaluation written to {eval_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure S1-style evaluation written to results_csv_clean/s_8/smooth/n_0.02/Results_20250117_154856__s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/smooth/n_0.02/Results_20250117_154856__s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/smooth/n_0.04/Results_20250117_155023__s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/smooth/n_0.04/Results_20250117_155023__s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/smooth/n_0.06/Results_20250117_155159__s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/smooth/n_0.06/Results_20250117_155159__s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/uniform/n_0.02/Results_20250117_154918_uniform_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/uniform/n_0.02/Results_20250117_154918_uniform_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/uniform/n_0.04/Results_20250117_155045_uniform_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/uniform/n_0.04/Results_20250117_155045_uniform_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/uniform/n_0.06/Results_20250117_155226_uniform_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/uniform/n_0.06/Results_20250117_155226_uniform_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/hamming/n_0.02/Results_20250117_155001_hamming_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/hamming/n_0.02/Results_20250117_155001_hamming_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/hamming/n_0.04/Results_20250117_155134_hamming_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/hamming/n_0.04/Results_20250117_155134_hamming_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/hamming/n_0.06/Results_20250117_155313_hamming_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/hamming/n_0.06/Results_20250117_155313_hamming_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/overall/n_0.02/Results_20250117_154939_overall_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/overall/n_0.02/Results_20250117_154939_overall_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/overall/n_0.04/Results_20250117_155112_overall_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/overall/n_0.04/Results_20250117_155112_overall_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/overall/n_0.06/Results_20250117_155248_overall_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_8/overall/n_0.06/Results_20250117_155248_overall_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/smooth/n_0.02/Results_20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/smooth/n_0.02/Results_20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/smooth/n_0.04/Results_20250117_154306__s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/smooth/n_0.04/Results_20250117_154306__s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/smooth/n_0.06/Results_20250117_154429__s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/smooth/n_0.06/Results_20250117_154429__s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/uniform/n_0.02/Results_20250117_154150_uniform_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/uniform/n_0.02/Results_20250117_154150_uniform_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/uniform/n_0.04/Results_20250117_154327_uniform_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/uniform/n_0.04/Results_20250117_154327_uniform_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/uniform/n_0.06/Results_20250117_154445_uniform_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/uniform/n_0.06/Results_20250117_154445_uniform_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/hamming/n_0.02/Results_20250117_154240_hamming_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/hamming/n_0.02/Results_20250117_154240_hamming_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/hamming/n_0.04/Results_20250117_154409_hamming_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/hamming/n_0.04/Results_20250117_154409_hamming_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/hamming/n_0.06/Results_20250117_154518_hamming_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/hamming/n_0.06/Results_20250117_154518_hamming_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/overall/n_0.02/Results_20250117_154214_overall_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/overall/n_0.02/Results_20250117_154214_overall_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/overall/n_0.04/Results_20250117_154348_overall_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/overall/n_0.04/Results_20250117_154348_overall_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/overall/n_0.06/Results_20250117_154501_overall_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_15/overall/n_0.06/Results_20250117_154501_overall_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/smooth/n_0.02/Results_20250117_154536__s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/smooth/n_0.02/Results_20250117_154536__s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/smooth/n_0.04/Results_20250117_154648__s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/smooth/n_0.04/Results_20250117_154648__s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/smooth/n_0.06/Results_20250117_154757__s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/smooth/n_0.06/Results_20250117_154757__s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/uniform/n_0.02/Results_20250117_154553_uniform_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/uniform/n_0.02/Results_20250117_154553_uniform_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/uniform/n_0.04/Results_20250117_154705_uniform_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/uniform/n_0.04/Results_20250117_154705_uniform_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/uniform/n_0.06/Results_20250117_154812_uniform_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/uniform/n_0.06/Results_20250117_154812_uniform_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/hamming/n_0.02/Results_20250117_154630_hamming_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/hamming/n_0.02/Results_20250117_154630_hamming_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/hamming/n_0.04/Results_20250117_154739_hamming_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/hamming/n_0.04/Results_20250117_154739_hamming_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/hamming/n_0.06/Results_20250117_154842_hamming_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/hamming/n_0.06/Results_20250117_154842_hamming_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/overall/n_0.02/Results_20250117_154610_overall_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/overall/n_0.02/Results_20250117_154610_overall_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/overall/n_0.04/Results_20250117_154722_overall_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/overall/n_0.04/Results_20250117_154722_overall_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/overall/n_0.06/Results_20250117_154827_overall_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_figS1_Dw.txt\n",
      "Figure S1-style evaluation written to results_csv_clean/s_25/overall/n_0.06/Results_20250117_154827_overall_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_figS1_Dkl.txt\n"
     ]
    }
   ],
   "source": [
    "run_evaluation_figS1(cutoff=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ground truth to results_csv_clean/s_8/ground_truth_signatures_s_8.csv\n",
      "Saved ground truth to results_csv_clean/s_15/ground_truth_signatures_s_15.csv\n",
      "Saved ground truth to results_csv_clean/s_25/ground_truth_signatures_s_25.csv\n",
      "Evaluation written to results_csv_clean/s_8/smooth/n_0.02/Results_20250117_154856__s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/smooth/n_0.02/Results_20250117_154856__s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/smooth/n_0.04/Results_20250117_155023__s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/smooth/n_0.04/Results_20250117_155023__s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/smooth/n_0.06/Results_20250117_155159__s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/smooth/n_0.06/Results_20250117_155159__s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/uniform/n_0.02/Results_20250117_154918_uniform_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/uniform/n_0.02/Results_20250117_154918_uniform_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/uniform/n_0.04/Results_20250117_155045_uniform_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/uniform/n_0.04/Results_20250117_155045_uniform_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/uniform/n_0.06/Results_20250117_155226_uniform_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/uniform/n_0.06/Results_20250117_155226_uniform_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/hamming/n_0.02/Results_20250117_155001_hamming_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/hamming/n_0.02/Results_20250117_155001_hamming_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/hamming/n_0.04/Results_20250117_155134_hamming_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/hamming/n_0.04/Results_20250117_155134_hamming_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/hamming/n_0.06/Results_20250117_155313_hamming_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/hamming/n_0.06/Results_20250117_155313_hamming_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/overall/n_0.02/Results_20250117_154939_overall_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/overall/n_0.02/Results_20250117_154939_overall_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/overall/n_0.04/Results_20250117_155112_overall_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/overall/n_0.04/Results_20250117_155112_overall_s_8_n_0.04__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_8/overall/n_0.06/Results_20250117_155248_overall_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_8/overall/n_0.06/Results_20250117_155248_overall_s_8_n_0.06__GRCh37_98_86_39_22a_43_96_37_54/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/smooth/n_0.02/Results_20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/smooth/n_0.02/Results_20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/smooth/n_0.04/Results_20250117_154306__s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/smooth/n_0.04/Results_20250117_154306__s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/smooth/n_0.06/Results_20250117_154429__s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/smooth/n_0.06/Results_20250117_154429__s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/uniform/n_0.02/Results_20250117_154150_uniform_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/uniform/n_0.02/Results_20250117_154150_uniform_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/uniform/n_0.04/Results_20250117_154327_uniform_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/uniform/n_0.04/Results_20250117_154327_uniform_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/uniform/n_0.06/Results_20250117_154445_uniform_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/uniform/n_0.06/Results_20250117_154445_uniform_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/hamming/n_0.02/Results_20250117_154240_hamming_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/hamming/n_0.02/Results_20250117_154240_hamming_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/hamming/n_0.04/Results_20250117_154409_hamming_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/hamming/n_0.04/Results_20250117_154409_hamming_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/hamming/n_0.06/Results_20250117_154518_hamming_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/hamming/n_0.06/Results_20250117_154518_hamming_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/overall/n_0.02/Results_20250117_154214_overall_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/overall/n_0.02/Results_20250117_154214_overall_s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/overall/n_0.04/Results_20250117_154348_overall_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/overall/n_0.04/Results_20250117_154348_overall_s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_15/overall/n_0.06/Results_20250117_154501_overall_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_15/overall/n_0.06/Results_20250117_154501_overall_s_15_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/smooth/n_0.02/Results_20250117_154536__s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/smooth/n_0.02/Results_20250117_154536__s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/smooth/n_0.04/Results_20250117_154648__s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/smooth/n_0.04/Results_20250117_154648__s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/smooth/n_0.06/Results_20250117_154757__s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/smooth/n_0.06/Results_20250117_154757__s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/uniform/n_0.02/Results_20250117_154553_uniform_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/uniform/n_0.02/Results_20250117_154553_uniform_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/uniform/n_0.04/Results_20250117_154705_uniform_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/uniform/n_0.04/Results_20250117_154705_uniform_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/uniform/n_0.06/Results_20250117_154812_uniform_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/uniform/n_0.06/Results_20250117_154812_uniform_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/hamming/n_0.02/Results_20250117_154630_hamming_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/hamming/n_0.02/Results_20250117_154630_hamming_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/hamming/n_0.04/Results_20250117_154739_hamming_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/hamming/n_0.04/Results_20250117_154739_hamming_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/hamming/n_0.06/Results_20250117_154842_hamming_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/hamming/n_0.06/Results_20250117_154842_hamming_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/overall/n_0.02/Results_20250117_154610_overall_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/overall/n_0.02/Results_20250117_154610_overall_s_25_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/overall/n_0.04/Results_20250117_154722_overall_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/overall/n_0.04/Results_20250117_154722_overall_s_25_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n",
      "Evaluation written to results_csv_clean/s_25/overall/n_0.06/Results_20250117_154827_overall_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dw/evaluation_Dw.txt\n",
      "Evaluation written to results_csv_clean/s_25/overall/n_0.06/Results_20250117_154827_overall_s_25_n_0.06__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93_26_12_22b_17a_9_25_55_89_13_46/Dkl/evaluation_Dkl.txt\n"
     ]
    }
   ],
   "source": [
    "# 4) Save ground-truth signatures from your known “file_map”\n",
    "COSMIC_SIGNATURES_PATH = \"cosmic_signatures/COSMIC_v3.4_SBS_GRCh37.txt\"\n",
    "save_ground_truth_signatures(COSMIC_SIGNATURES_PATH)\n",
    "\n",
    "# 5) Evaluate\n",
    "run_evaluation(cutoff=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluation_figS1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of est_sigs: (96, 8)\n",
      "Shape of true_sigs: (96, 8)\n",
      "Evaluation Results:\n",
      "Number of ground-truth signatures (n_gt): 8\n",
      "Number of detected signatures (n_detected): 8\n",
      "True Positives (TP): 8\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 5\n",
      "Precision: 1.0\n",
      "Recall: 0.62\n",
      "F1 Score: 0.76\n",
      "Evaluation Results medois :\n",
      "Number of ground-truth signatures (n_gt): 8\n",
      "Number of detected signatures (n_detected): 8\n",
      "True Positives (TP): 8\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 5\n",
      "Precision: 1.0\n",
      "Recall: 0.62\n",
      "F1 Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "est_sigs_path = \"results_csv_clean/s_8/hamming/n_0.02/Results_20250117_155001_hamming_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/Dkl_0.csv\"\n",
    "est_sigs2_path =\"results_csv_clean/s_8/hamming/n_0.02/Results_20250117_155001_hamming_s_8_n_0.02__GRCh37_98_86_39_22a_43_96_37_54/Dkl/medoids_Dkl.csv\"\n",
    "\n",
    "true_sigs_path = \"/Users/lorispodevyn/Documents/COURSES/PROJECT_MASTER_1/Code/Mutational_Signatures/results_csv_clean/s_8/ground_truth_signatures_s_8.csv\"\n",
    "\n",
    "# Load CSV data\n",
    "est_sigs = pd.read_csv(est_sigs_path, header=None).values\n",
    "est_sigs2 = pd.read_csv(est_sigs2_path, header=None).values\n",
    "true_sigs = pd.read_csv(true_sigs_path, header=None).values\n",
    "true_sigs = true_sigs[1:, 1:]\n",
    "\n",
    "\n",
    "print(\"Shape of est_sigs:\", est_sigs.shape)\n",
    "print(\"Shape of true_sigs:\", true_sigs.shape)\n",
    "\n",
    "# Evaluate\n",
    "results = evaluation_figS1(true_sigs, est_sigs,cutoff=0.5)\n",
    "results2 = evaluation_figS1(true_sigs, est_sigs2,cutoff=0.5)\n",
    "# Print all the results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Number of ground-truth signatures (n_gt): {results[0]}\")\n",
    "print(f\"Number of detected signatures (n_detected): {results[1]}\")\n",
    "print(f\"True Positives (TP): {results[2]}\")\n",
    "print(f\"False Positives (FP): {results[3]}\")\n",
    "print(f\"False Negatives (FN): {results[4]}\")\n",
    "print(f\"Precision: {results[5]}\")\n",
    "print(f\"Recall: {results[6]}\")\n",
    "print(f\"F1 Score: {results[7]}\")\n",
    "\n",
    "\n",
    "print(\"Evaluation Results medois :\")\n",
    "print(f\"Number of ground-truth signatures (n_gt): {results2[0]}\")\n",
    "print(f\"Number of detected signatures (n_detected): {results2[1]}\")\n",
    "print(f\"True Positives (TP): {results2[2]}\")\n",
    "print(f\"False Positives (FP): {results2[3]}\")\n",
    "print(f\"False Negatives (FN): {results2[4]}\")\n",
    "print(f\"Precision: {results2[5]}\")\n",
    "print(f\"Recall: {results2[6]}\")\n",
    "print(f\"F1 Score: {results2[7]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
