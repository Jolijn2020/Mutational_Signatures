{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "import pyclustering\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from pyclustering.utils.metric import distance_metric, type_metric\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    parsed_data = {}\n",
    "    for key, value in data.items():\n",
    "        # Check if the value is a list\n",
    "        if isinstance(value, list):\n",
    "            try:\n",
    "                # Attempt to convert to a NumPy array\n",
    "                subfield = np.array(value)\n",
    "\n",
    "                if subfield.ndim == 1:  # 1D array\n",
    "                    parsed_data[key] = pd.Series(value)\n",
    "                elif subfield.ndim == 2:  # 2D array\n",
    "                    parsed_data[key] = pd.DataFrame(value)\n",
    "                else:  # Higher-dimensional array\n",
    "                    parsed_data[key] = {i: pd.DataFrame(subarray) for i, subarray in enumerate(subfield)}\n",
    "            except ValueError:\n",
    "                # If conversion fails, handle as a dictionary of DataFrames\n",
    "                parsed_data[key] = {i: pd.DataFrame(subarray) for i, subarray in enumerate(value) if isinstance(subarray, list)}\n",
    "        else:\n",
    "            # For non-list data, store it as-is\n",
    "            parsed_data[key] = value\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sig_profiler_extractor\n",
    "def evaluation(true_sigs, est_sigs, cutoff=None, dist=\"cos\", verbose=False):\n",
    "    if true_sigs.shape[1] >= est_sigs.shape[1]:\n",
    "        mat1 = est_sigs\n",
    "        mat2 = true_sigs\n",
    "    else:\n",
    "        mat1 = true_sigs\n",
    "        mat2 = est_sigs\n",
    "\n",
    "    if dist == \"cos\":\n",
    "        con_mat = cdist(mat1.T, mat2.T, \"cosine\")\n",
    "    elif dist == \"cor\":\n",
    "        con_mat = cdist(mat1.T, mat2.T, \"correlation\")\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(con_mat)\n",
    "    con_mat = 1 - con_mat  # Convert distance -> similarity\n",
    "\n",
    "    idxPair = {}\n",
    "    true_positives = 0\n",
    "    for x, y in zip(row_ind, col_ind):\n",
    "        idxPair[x] = y\n",
    "        if con_mat[x, y] >= cutoff:\n",
    "            true_positives += 1\n",
    "\n",
    "    computedFalsePositives = mat1.shape[1] - true_positives\n",
    "    computedFalseNegatives = computedFalsePositives\n",
    "\n",
    "    if true_sigs.shape[1] >= est_sigs.shape[1]:\n",
    "        baseFalsePositives = 0\n",
    "        baseFalseNegatives = true_sigs.shape[1] - est_sigs.shape[1]\n",
    "    else:\n",
    "        baseFalsePositives = est_sigs.shape[1] - true_sigs.shape[1]\n",
    "        baseFalseNegatives = 0\n",
    "\n",
    "    false_positives = baseFalsePositives + computedFalsePositives\n",
    "    false_negatives = baseFalseNegatives + computedFalseNegatives\n",
    "    number_of_ground_truth_signatures = true_sigs.shape[1]\n",
    "    number_of_detected_signature = est_sigs.shape[1]\n",
    "\n",
    "    try:\n",
    "        precision = round(true_positives / (true_positives + false_positives), 2)\n",
    "        recall = round(true_positives / (true_positives + false_negatives), 2)\n",
    "        f1_score = round(2 * precision * recall / (precision + recall), 2)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1_score = 0\n",
    "\n",
    "    return (\n",
    "        number_of_ground_truth_signatures,\n",
    "        number_of_detected_signature,\n",
    "        true_positives,\n",
    "        false_positives,\n",
    "        false_negatives,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        idxPair,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pam_cosine_clustering(data, n_clusters=None, random_seed=seed):\n",
    "    \"\"\"\n",
    "    Performs PAM clustering using cosine distance on given signature data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray or pd.DataFrame): Signature data, where each column is a signature vector.\n",
    "    - n_clusters (int): Number of clusters to form.\n",
    "    - random_seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - clusters (list): List of clusters, where each cluster is a list of indices.\n",
    "    - medoid_signatures (np.ndarray): Array of medoid signature vectors.\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)  # Set random seed for reproducibility\n",
    "\n",
    "    # Ensure the input is a NumPy array\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.to_numpy()  # Convert DataFrame to NumPy array\n",
    "\n",
    "    # Transpose the data to treat columns as signatures\n",
    "    data = data.T  # Shape: (num_signatures, num_bins)\n",
    "    num_signatures, num_bins = data.shape\n",
    "\n",
    "    if n_clusters > num_signatures:\n",
    "        raise ValueError(\n",
    "            f\"n_clusters ({n_clusters}) is greater than total signatures ({num_signatures}).\"\n",
    "        )\n",
    "    \n",
    "    # Define the cosine distance metric\n",
    "    def custom_cosine_distance(vec_a, vec_b):\n",
    "        return cdist([vec_a], [vec_b], \"cosine\")[0, 0]\n",
    "\n",
    "    metric = distance_metric(type_metric.USER_DEFINED, func=custom_cosine_distance)\n",
    "\n",
    "    # Perform k-medoids clustering\n",
    "    # Pick random distinct indices as initial medoids\n",
    "    initial_medoids = random.sample(range(num_signatures), n_clusters)\n",
    "\n",
    "    # Convert data to a list of lists (required by pyclustering)\n",
    "    data_list = data.tolist()\n",
    "\n",
    "    # Run k-medoids (PAM)\n",
    "    kmedoids_instance = kmedoids(\n",
    "        data=data_list,\n",
    "        initial_index_medoids=initial_medoids,\n",
    "        metric=metric\n",
    "    )\n",
    "    kmedoids_instance.process()\n",
    "\n",
    "    # Retrieve clusters and medoids\n",
    "    clusters = kmedoids_instance.get_clusters()  # List of cluster indices\n",
    "    medoids = kmedoids_instance.get_medoids()    # Indices of medoids\n",
    "\n",
    "    # Retrieve the actual medoid signature vectors\n",
    "    medoid_signatures = np.array([data[idx] for idx in medoids])\n",
    "\n",
    "    return clusters, medoid_signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____Load the csvs to be clustered_____\n",
    "\n",
    "def load_and_cluster(folder_path, n_clusters, output_csv_path, random_seed=42):\n",
    "    \"\"\"\n",
    "    Loads signature data from a given folder, performs PAM clustering, and saves results.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing CSV files (e.g., `Dkl` or `Dw`).\n",
    "    - n_clusters (int): Number of clusters for PAM clustering.\n",
    "    - output_csv_path (str): Path to save the clustering results (medoid signatures).\n",
    "    - random_seed (int): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - clusters (list): List of clusters.\n",
    "    - medoid_signatures (np.ndarray): Array of medoid signature vectors.\n",
    "    \"\"\"\n",
    "    # Load all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    all_signatures = []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(folder_path, csv_file)\n",
    "        # Load CSV as a matrix\n",
    "        signature_matrix = pd.read_csv(csv_path, header=None).to_numpy()\n",
    "        all_signatures.append(signature_matrix)\n",
    "\n",
    "    # Concatenate all loaded signatures into one NumPy array (columns = signatures)\n",
    "    combined_data = np.hstack(all_signatures)  # Shape: (num_bins, total_signatures)\n",
    "\n",
    "    # Perform PAM clustering\n",
    "    clusters, medoid_signatures = pam_cosine_clustering(\n",
    "        data=combined_data,\n",
    "        n_clusters=n_clusters,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "\n",
    "    # Save medoid signatures to CSV\n",
    "    pd.DataFrame(medoid_signatures.T).to_csv(output_csv_path, index=False, header=False)\n",
    "    print(f\"Medoid signatures saved to: {output_csv_path}\")\n",
    "\n",
    "    return clusters, medoid_signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _______Signature Matrices EXTRACTION _______\n",
    "\n",
    "results_dir = \"results_17_01\"   # Path to your JSON file's folder\n",
    "results_csv_dir = \"results_csv\" \n",
    "\n",
    "# Create the results_csv directory if it doesn't exist\n",
    "os.makedirs(results_csv_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the s_15, s_8, and s_25 folders exist\n",
    "for parent_folder in [\"s_15\", \"s_8\", \"s_25\"]:\n",
    "    parent_folder_path = os.path.join(results_csv_dir, parent_folder)\n",
    "    os.makedirs(parent_folder_path, exist_ok=True)\n",
    "\n",
    "    # Create distance metric subfolders inside each parent folder\n",
    "    for distance in [\"smooth\", \"uniform\", \"hamming\", \"overall\"]:\n",
    "        distance_folder_path = os.path.join(parent_folder_path, distance)\n",
    "        os.makedirs(distance_folder_path, exist_ok=True)\n",
    "\n",
    "        # Create noise level subfolders under each distance metric folder\n",
    "        for noise_level in [\"n_0.02\", \"n_0.04\", \"n_0.06\"]:\n",
    "            noise_folder_path = os.path.join(distance_folder_path, noise_level)\n",
    "            os.makedirs(noise_folder_path, exist_ok=True)\n",
    "\n",
    "# Process each JSON file in the results directory\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "        \n",
    "        # Parse the JSON data\n",
    "        parsed_data = load_data(file_path)\n",
    "\n",
    "        # Extract the main folder name based on \"s_15\", \"s_8\", \"s_25\"\n",
    "        if \"s_15\" in filename:\n",
    "            parent_folder = \"s_15\"\n",
    "        elif \"s_8\" in filename:\n",
    "            parent_folder = \"s_8\"\n",
    "        elif \"s_25\" in filename:\n",
    "            parent_folder = \"s_25\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown parent folder for file: {filename}\")\n",
    "\n",
    "        # Extract the distance metric from the filename\n",
    "        distance_match = next(\n",
    "            (distance for distance in [\"uniform\", \"hamming\", \"overall\"] if distance in filename), \"smooth\"\n",
    "        )\n",
    "        distance = distance_match  # Default to \"smooth\" if no distance metric is found\n",
    "\n",
    "        # Extract the noise level from the filename\n",
    "        if \"n_0.02\" in filename:\n",
    "            noise_level = \"n_0.02\"\n",
    "        elif \"n_0.04\" in filename:\n",
    "            noise_level = \"n_0.04\"\n",
    "        elif \"n_0.06\" in filename:\n",
    "            noise_level = \"n_0.06\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown noise level for file: {filename}\")\n",
    "\n",
    "        # Create the parent folder, distance folder, and noise level folder\n",
    "        parent_folder_path = os.path.join(results_csv_dir, parent_folder)\n",
    "        distance_folder_path = os.path.join(parent_folder_path, distance)\n",
    "        noise_folder_path = os.path.join(distance_folder_path, noise_level)\n",
    "        os.makedirs(noise_folder_path, exist_ok=True)\n",
    "\n",
    "        # Get the subfolder name (without .json)\n",
    "        subfolder_name = os.path.splitext(filename)[0]\n",
    "        subfolder_name = subfolder_name[subfolder_name.find('_')+1:] if '_' in subfolder_name else subfolder_name\n",
    "        subfolder_path = os.path.join(noise_folder_path, subfolder_name)\n",
    "        \n",
    "        # Create the subfolder\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        # Create separate subfolders for Dw and Dkl\n",
    "        dw_folder_path = os.path.join(subfolder_path, \"Dw\")\n",
    "        dkl_folder_path = os.path.join(subfolder_path, \"Dkl\")\n",
    "        os.makedirs(dw_folder_path, exist_ok=True)\n",
    "        os.makedirs(dkl_folder_path, exist_ok=True)\n",
    "\n",
    "        # Save all Dw matrices to CSV\n",
    "        if 'all_Dw' in parsed_data:\n",
    "            for i, dw_matrix in parsed_data['all_Dw'].items():\n",
    "                dw_csv_path = os.path.join(dw_folder_path, f\"Dw_{i}.csv\")\n",
    "                dw_matrix.to_csv(dw_csv_path, index=False, header=False)  # Exclude index and header\n",
    "\n",
    "        # Save all Dkl matrices to CSV\n",
    "        if 'all_Dkl' in parsed_data:\n",
    "            for i, dkl_matrix in parsed_data['all_Dkl'].items():\n",
    "                dkl_csv_path = os.path.join(dkl_folder_path, f\"Dkl_{i}.csv\")\n",
    "                dkl_matrix.to_csv(dkl_csv_path, index=False, header=False)  # Exclude index and header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_folders(results_csv_dir, s_folders, distances, n_folders, subfolders):\n",
    "    \"\"\"\n",
    "    Traverses the folder structure and yields paths for processing.\n",
    "\n",
    "    Parameters:\n",
    "    - results_csv_dir (str): Root directory of the results.\n",
    "    - s_folders (list): List of s_* folders (e.g., [\"s_15\", \"s_8\", \"s_25\"]).\n",
    "    - distances (list): List of distance metrics (e.g., [\"smooth\", \"uniform\", \"hamming\", \"overall\"]).\n",
    "    - n_folders (list): List of n_* noise level folders (e.g., [\"n_0.02\", \"n_0.04\", \"n_0.08\"]).\n",
    "    - subfolders (list): List of subfolders to process (e.g., [\"Dw\", \"Dkl\"]).\n",
    "\n",
    "    Yields:\n",
    "    - s_folder (str): Current s_* folder name.\n",
    "    - distance (str): Current distance metric.\n",
    "    - n_folder (str): Current noise level folder.\n",
    "    - run_folder (str): Current run folder name.\n",
    "    - subfolder_path (str): Path to the current subfolder (e.g., Dw or Dkl).\n",
    "    \"\"\"\n",
    "    for s_folder in s_folders:\n",
    "        s_folder_path = os.path.join(results_csv_dir, s_folder)\n",
    "\n",
    "        for distance in distances:\n",
    "            distance_folder_path = os.path.join(s_folder_path, distance)\n",
    "\n",
    "            for n_folder in n_folders:\n",
    "                n_folder_path = os.path.join(distance_folder_path, n_folder)\n",
    "\n",
    "                for run_folder in os.listdir(n_folder_path):\n",
    "                    run_folder_path = os.path.join(n_folder_path, run_folder)\n",
    "\n",
    "                    for subfolder in subfolders:\n",
    "                        subfolder_path = os.path.join(run_folder_path, subfolder)\n",
    "                        if os.path.exists(subfolder_path):\n",
    "                            yield s_folder, distance, n_folder, run_folder, subfolder_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medoid signatures saved to: results_csv/s_15/smooth/n_0.02/20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/medoids_Dw.csv\n",
      "Clustering processed for: results_csv/s_15/smooth/n_0.02/20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw\n",
      "Medoid signatures saved to: results_csv/s_15/smooth/n_0.02/20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl/medoids_Dkl.csv\n",
      "Clustering processed for: results_csv/s_15/smooth/n_0.02/20250117_154127__s_15_n_0.02__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dkl\n",
      "Medoid signatures saved to: results_csv/s_15/smooth/n_0.04/20250117_154306__s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw/medoids_Dw.csv\n",
      "Clustering processed for: results_csv/s_15/smooth/n_0.04/20250117_154306__s_15_n_0.04__GRCh37_98_86_39_22a_43_96_37_54_87_17b_21_99_3_33_93/Dw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedoids_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(subfolder_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Perform clustering\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m clusters, medoid_signatures \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_cluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust dynamically if needed\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustering processed for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[131], line 31\u001b[0m, in \u001b[0;36mload_and_cluster\u001b[0;34m(folder_path, n_clusters, output_csv_path, random_seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(all_signatures)  \u001b[38;5;66;03m# Shape: (num_bins, total_signatures)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Perform PAM clustering\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m clusters, medoid_signatures \u001b[38;5;241m=\u001b[39m \u001b[43mpam_cosine_clustering\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Save medoid signatures to CSV\u001b[39;00m\n\u001b[1;32m     38\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(medoid_signatures\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mto_csv(output_csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[130], line 48\u001b[0m, in \u001b[0;36mpam_cosine_clustering\u001b[0;34m(data, n_clusters, random_seed)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Run k-medoids (PAM)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m kmedoids_instance \u001b[38;5;241m=\u001b[39m kmedoids(\n\u001b[1;32m     44\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata_list,\n\u001b[1;32m     45\u001b[0m     initial_index_medoids\u001b[38;5;241m=\u001b[39minitial_medoids,\n\u001b[1;32m     46\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[43mkmedoids_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Retrieve clusters and medoids\u001b[39;00m\n\u001b[1;32m     51\u001b[0m clusters \u001b[38;5;241m=\u001b[39m kmedoids_instance\u001b[38;5;241m.\u001b[39mget_clusters()  \u001b[38;5;66;03m# List of cluster indices\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyclustering/cluster/kmedoids.py:164\u001b[0m, in \u001b[0;36mkmedoids.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m     current_deviation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_clusters()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (changes \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__tolerance) \u001b[38;5;129;01mand\u001b[39;00m (iterations \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__itermax):\n\u001b[0;32m--> 164\u001b[0m     swap_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__swap_medoids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swap_cost \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    167\u001b[0m         previous_deviation \u001b[38;5;241m=\u001b[39m current_deviation\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyclustering/cluster/kmedoids.py:356\u001b[0m, in \u001b[0;36mkmedoids.__swap_medoids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (candidate_medoid_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__medoid_indexes) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__distance_first_medoid[candidate_medoid_index] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m swap_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__calculate_swap_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_medoid_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_cluster\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swap_cost \u001b[38;5;241m<\u001b[39m optimal_swap_cost:\n\u001b[1;32m    358\u001b[0m     optimal_swap_cost \u001b[38;5;241m=\u001b[39m swap_cost\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyclustering/cluster/kmedoids.py:384\u001b[0m, in \u001b[0;36mkmedoids.__calculate_swap_cost\u001b[0;34m(self, index_candidate, cluster_index)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_point \u001b[38;5;241m==\u001b[39m index_candidate:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m candidate_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__distance_calculator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_candidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__labels[index_point] \u001b[38;5;241m==\u001b[39m cluster_index:\n\u001b[1;32m    386\u001b[0m     cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(candidate_distance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__distance_second_medoid[index_point]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__distance_first_medoid[index_point]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyclustering/cluster/kmedoids.py:291\u001b[0m, in \u001b[0;36mkmedoids.__create_distance_calculator.<locals>.<lambda>\u001b[0;34m(index1, index2)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"!\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m@brief Creates distance calculator in line with algorithms parameters.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m@return (callable) Distance calculator.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m index1, index2: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__metric\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pointer_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pointer_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pointer_data, numpy\u001b[38;5;241m.\u001b[39mmatrix):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pyclustering/utils/metric.py:132\u001b[0m, in \u001b[0;36mdistance_metric.__call__\u001b[0;34m(self, point1, point2)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, point1, point2):\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"!\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    @brief Calculates distance between two points.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__calculator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 31\u001b[0m, in \u001b[0;36mpam_cosine_clustering.<locals>.custom_cosine_distance\u001b[0;34m(vec_a, vec_b)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_cosine_distance\u001b[39m(vec_a, vec_b):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvec_a\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mvec_b\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/scipy/spatial/distance.py:2909\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;124;03mCompute distance between each pair of the two collections of inputs.\u001b[39;00m\n\u001b[1;32m   2611\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2901\u001b[0m \n\u001b[1;32m   2902\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;66;03m# You can also call this as:\u001b[39;00m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;66;03m#     Y = cdist(XA, XB, 'test_abc')\u001b[39;00m\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;66;03m# where 'abc' is the metric being tested.  This computes the distance\u001b[39;00m\n\u001b[1;32m   2906\u001b[0m \u001b[38;5;66;03m# between all pairs of vectors in XA and XB using the distance metric 'abc'\u001b[39;00m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;66;03m# but with a more succinct, verifiable, but less efficient implementation.\u001b[39;00m\n\u001b[0;32m-> 2909\u001b[0m XA \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2910\u001b[0m XB \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(XB)\n\u001b[1;32m   2912\u001b[0m s \u001b[38;5;241m=\u001b[39m XA\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "s_folders = [\"s_8\", \"s_15\", \"s_25\"]\n",
    "distances = [\"smooth\", \"uniform\", \"hamming\", \"overall\"]\n",
    "n_folders = [\"n_0.02\", \"n_0.04\", \"n_0.08\"]\n",
    "subfolders = [\"Dw\", \"Dkl\"]\n",
    "\n",
    "# Map s_folders to cluster numbers\n",
    "cluster_mapping = {\n",
    "    \"s_8\": 8,\n",
    "    \"s_15\": 15,\n",
    "    \"s_25\": 25\n",
    "}\n",
    "\n",
    "# Iterate using the reusable function\n",
    "for s_folder, distance, n_folder, run_folder, subfolder_path in traverse_folders(\n",
    "    results_csv_dir=results_csv_dir,\n",
    "    s_folders=s_folders,\n",
    "    distances=distances,\n",
    "    n_folders=n_folders,\n",
    "    subfolders=subfolders\n",
    "):\n",
    "    # Dynamically determine the number of clusters\n",
    "    n_clusters = cluster_mapping[s_folder]\n",
    "\n",
    "    # Define output path for clustering results\n",
    "    output_csv_path = os.path.join(subfolder_path, f\"medoids_{os.path.basename(subfolder_path)}.csv\")\n",
    "\n",
    "    # Perform clustering\n",
    "    clusters, medoid_signatures = load_and_cluster(\n",
    "        folder_path=subfolder_path,\n",
    "        n_clusters=n_clusters,  # Use dynamically determined cluster number\n",
    "        output_csv_path=output_csv_path,\n",
    "        random_seed=seed\n",
    "    )\n",
    "\n",
    "    print(f\"Clustering processed for: {subfolder_path} with {n_clusters} clusters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Signature Names: ['SBS10a', 'SBS56', 'SBS10d', 'SBS52', 'SBS36', 'SBS91', 'SBS45', 'SBS38']\n",
      "Filtered Signatures:\n",
      "                SBS10a     SBS56    SBS10d     SBS52     SBS36     SBS91  \\\n",
      "Type                                                                      \n",
      "A[C>A]A  2.190170e-03  0.012597  0.010114  0.015196  0.025194  0.002945   \n",
      "A[C>A]C  1.770137e-03  0.015697  0.018446  0.006538  0.008318  0.052997   \n",
      "A[C>A]G  1.500120e-04  0.000206  0.000727  0.004139  0.002239  0.000204   \n",
      "A[C>A]T  1.700132e-02  0.022995  0.014197  0.009238  0.017896  0.000131   \n",
      "A[C>G]A  2.230000e-16  0.000418  0.000129  0.001730  0.001840  0.000243   \n",
      "...               ...       ...       ...       ...       ...       ...   \n",
      "T[T>C]T  3.250252e-03  0.000285  0.007555  0.002000  0.002799  0.001274   \n",
      "T[T>G]A  2.690209e-03  0.009458  0.019898  0.001430  0.000788  0.005955   \n",
      "T[T>G]C  2.230000e-16  0.000001  0.000738  0.001120  0.000744  0.000143   \n",
      "T[T>G]G  2.160000e-05  0.000101  0.003148  0.001480  0.000902  0.000628   \n",
      "T[T>G]T  1.890147e-02  0.028994  0.014528  0.002529  0.001810  0.001009   \n",
      "\n",
      "            SBS45     SBS38  \n",
      "Type                         \n",
      "A[C>A]A  0.009108  0.012795  \n",
      "A[C>A]C  0.002849  0.010096  \n",
      "A[C>A]G  0.001660  0.001899  \n",
      "A[C>A]T  0.009638  0.008846  \n",
      "A[C>G]A  0.003109  0.002689  \n",
      "...           ...       ...  \n",
      "T[T>C]T  0.000545  0.003059  \n",
      "T[T>G]A  0.000285  0.001170  \n",
      "T[T>G]C  0.000135  0.000903  \n",
      "T[T>G]G  0.000708  0.001899  \n",
      "T[T>G]T  0.000724  0.005288  \n",
      "\n",
      "[96 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# _______COSMIC signatures exctraction_______\n",
    "# TODO change this function when I know about the cosmic signatures we use\n",
    "# Define the filename\n",
    "filename = \"Results_20250116_194705_overall_s_8_n_0.02__GRCh37_10a_56_10d_52_36_91_45_38.json\"\n",
    "\n",
    "# Step 1: Extract signature names after \"GRCh37\"\n",
    "signature_portion = re.search(r\"GRCh37_([\\w\\d_]+)\", filename).group(1)  # Extracts \"10a_56_10d_52_36_91_45_38\"\n",
    "signature_ids = signature_portion.split(\"_\")  # Splits into ['10a', '56', '10d', '52', ...]\n",
    "\n",
    "# Prepend \"SBS\" to each signature ID\n",
    "signature_names = [f\"SBS{id}\" for id in signature_ids]\n",
    "print(\"Extracted Signature Names:\", signature_names)\n",
    "\n",
    "# Step 2: Load the COSMIC signatures file\n",
    "cosmic_signatures_path = \"cosmic_signatures/COSMIC_v3.4_SBS_GRCh37.txt\"\n",
    "signatures_df = pd.read_csv(cosmic_signatures_path, sep=\"\\t\")\n",
    "\n",
    "# Set the index if the \"Type\" column exists\n",
    "if \"Type\" in signatures_df.columns:\n",
    "    signatures_df.set_index(\"Type\", inplace=True)\n",
    "\n",
    "# Step 3: Filter the signatures corresponding to the extracted names\n",
    "filtered_signatures = signatures_df[signature_names]\n",
    "print(\"Filtered Signatures:\\n\", filtered_signatures)\n",
    "\n",
    "# Convert the filtered signatures to a NumPy array (if needed)\n",
    "true_signatures = filtered_signatures.to_numpy()\n",
    "true_sig_names = filtered_signatures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_175345_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dw/evaluation_Dw.txt\n",
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_175345_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dkl/evaluation_Dkl.txt\n",
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_173647_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dw/evaluation_Dw.txt\n",
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_173647_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dkl/evaluation_Dkl.txt\n",
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_180932_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dw/evaluation_Dw.txt\n",
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_180932_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dkl/evaluation_Dkl.txt\n",
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_180301_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dw/evaluation_Dw.txt\n",
      "Evaluation completed: results_csv/s_15/smooth/n_0.02/20250116_180301_smooth_s_15_n_0.02_GRCh37_10a_56_10d_52_36_91_45_38_10c_14_18_7b_7a_23_19/Dkl/evaluation_Dkl.txt\n"
     ]
    }
   ],
   "source": [
    "# Use traverse_folders to navigate the structure and evaluate medoids\n",
    "for s_folder, distance, n_folder, run_folder, subfolder_path in traverse_folders(\n",
    "    results_csv_dir=results_csv_dir,\n",
    "    s_folders=[\"s_15\", \"s_8\", \"s_25\"],\n",
    "    distances=[\"smooth\", \"uniform\", \"hamming\", \"overall\"],\n",
    "    n_folders=[\"n_0.02\", \"n_0.04\", \"n_0.08\"],\n",
    "    subfolders=[\"Dw\", \"Dkl\"]\n",
    "):\n",
    "    # Path to the medoids CSV\n",
    "    medoids_csv_path = os.path.join(subfolder_path, f\"medoids_{os.path.basename(subfolder_path)}.csv\")\n",
    "\n",
    "    # Check if the medoids CSV exists\n",
    "    if os.path.exists(medoids_csv_path):\n",
    "        # Load the medoid signatures\n",
    "        medoid_signatures = pd.read_csv(medoids_csv_path, header=None).to_numpy()\n",
    "        cut_off = 0.9\n",
    "\n",
    "        # Perform evaluation\n",
    "        eval_results = evaluation(\n",
    "            true_sigs=true_signatures,\n",
    "            est_sigs=medoid_signatures,\n",
    "            cutoff=cut_off,  # Adjust cutoff as needed\n",
    "            dist=\"cos\"\n",
    "        )\n",
    "\n",
    "        # Write evaluation results to a text file\n",
    "        eval_output_path = os.path.join(subfolder_path, f\"evaluation_{os.path.basename(subfolder_path)}.txt\")\n",
    "        with open(eval_output_path, \"w\") as f:\n",
    "            f.write(\"=== Evaluation Results ===\\n\")\n",
    "            f.write(f\"Cutoff Used: {cut_off}\\n\")  \n",
    "            f.write(f\"s_folder: {s_folder}\\n\")\n",
    "            f.write(f\"distance: {distance}\\n\")\n",
    "            f.write(f\"n_folder: {n_folder}\\n\")\n",
    "            f.write(f\"run_folder: {run_folder}\\n\")\n",
    "            f.write(f\"subfolder: {os.path.basename(subfolder_path)}\\n\")\n",
    "            f.write(f\"Number of Ground Truth Signatures: {eval_results[0]}\\n\")\n",
    "            f.write(f\"Number of Detected Signatures: {eval_results[1]}\\n\")\n",
    "            f.write(f\"True Positives: {eval_results[2]}\\n\")\n",
    "            f.write(f\"False Positives: {eval_results[3]}\\n\")\n",
    "            f.write(f\"False Negatives: {eval_results[4]}\\n\")\n",
    "            f.write(f\"Precision: {eval_results[5]}\\n\")\n",
    "            f.write(f\"Recall: {eval_results[6]}\\n\")\n",
    "            f.write(f\"F1 Score: {eval_results[7]}\\n\")\n",
    "            f.write(\"Index Pairs (Extracted -> Ground Truth):\\n\")\n",
    "            for extracted_idx, true_idx in eval_results[8].items():\n",
    "                f.write(f\"  Extracted {extracted_idx} -> Ground Truth {true_idx}\\n\")\n",
    "\n",
    "        print(f\"Evaluation completed: {eval_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
