{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "import pyclustering\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from pyclustering.utils.metric import distance_metric, type_metric\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    parsed_data = {}\n",
    "    for key, value in data.items():\n",
    "        # Check if the value is a list\n",
    "        if isinstance(value, list):\n",
    "            try:\n",
    "                # Attempt to convert to a NumPy array\n",
    "                subfield = np.array(value)\n",
    "\n",
    "                if subfield.ndim == 1:  # 1D array\n",
    "                    parsed_data[key] = pd.Series(value)\n",
    "                elif subfield.ndim == 2:  # 2D array\n",
    "                    parsed_data[key] = pd.DataFrame(value)\n",
    "                else:  # Higher-dimensional array\n",
    "                    parsed_data[key] = {i: pd.DataFrame(subarray) for i, subarray in enumerate(subfield)}\n",
    "            except ValueError:\n",
    "                # If conversion fails, handle as a dictionary of DataFrames\n",
    "                parsed_data[key] = {i: pd.DataFrame(subarray) for i, subarray in enumerate(value) if isinstance(subarray, list)}\n",
    "        else:\n",
    "            # For non-list data, store it as-is\n",
    "            parsed_data[key] = value\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sig_profiler_extractor\n",
    "def evaluation(true_sigs, est_sigs, cutoff=None, dist=\"cos\", verbose=False):\n",
    "    if true_sigs.shape[1] >= est_sigs.shape[1]:\n",
    "        mat1 = est_sigs\n",
    "        mat2 = true_sigs\n",
    "    else:\n",
    "        mat1 = true_sigs\n",
    "        mat2 = est_sigs\n",
    "\n",
    "    if dist == \"cos\":\n",
    "        con_mat = cdist(mat1.T, mat2.T, \"cosine\")\n",
    "    elif dist == \"cor\":\n",
    "        con_mat = cdist(mat1.T, mat2.T, \"correlation\")\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(con_mat)\n",
    "    con_mat = 1 - con_mat  # Convert distance -> similarity\n",
    "\n",
    "    idxPair = {}\n",
    "    true_positives = 0\n",
    "    for x, y in zip(row_ind, col_ind):\n",
    "        idxPair[x] = y\n",
    "        if con_mat[x, y] >= cutoff:\n",
    "            true_positives += 1\n",
    "\n",
    "    computedFalsePositives = mat1.shape[1] - true_positives\n",
    "    computedFalseNegatives = computedFalsePositives\n",
    "\n",
    "    if true_sigs.shape[1] >= est_sigs.shape[1]:\n",
    "        baseFalsePositives = 0\n",
    "        baseFalseNegatives = true_sigs.shape[1] - est_sigs.shape[1]\n",
    "    else:\n",
    "        baseFalsePositives = est_sigs.shape[1] - true_sigs.shape[1]\n",
    "        baseFalseNegatives = 0\n",
    "\n",
    "    false_positives = baseFalsePositives + computedFalsePositives\n",
    "    false_negatives = baseFalseNegatives + computedFalseNegatives\n",
    "    number_of_ground_truth_signatures = true_sigs.shape[1]\n",
    "    number_of_detected_signature = est_sigs.shape[1]\n",
    "\n",
    "    try:\n",
    "        precision = round(true_positives / (true_positives + false_positives), 2)\n",
    "        recall = round(true_positives / (true_positives + false_negatives), 2)\n",
    "        f1_score = round(2 * precision * recall / (precision + recall), 2)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1_score = 0\n",
    "\n",
    "    return (\n",
    "        number_of_ground_truth_signatures,\n",
    "        number_of_detected_signature,\n",
    "        true_positives,\n",
    "        false_positives,\n",
    "        false_negatives,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1_score,\n",
    "        idxPair,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pam_cosine_clustering(data, n_clusters=None, random_seed=seed):\n",
    "    \"\"\"\n",
    "    Performs PAM clustering using cosine distance on given signature data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray or pd.DataFrame): Signature data, where each column is a signature vector.\n",
    "    - n_clusters (int): Number of clusters to form.\n",
    "    - random_seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - clusters (list): List of clusters, where each cluster is a list of indices.\n",
    "    - medoid_signatures (np.ndarray): Array of medoid signature vectors.\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)  # Set random seed for reproducibility\n",
    "\n",
    "    # Ensure the input is a NumPy array\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.to_numpy()  # Convert DataFrame to NumPy array\n",
    "\n",
    "    # Transpose the data to treat columns as signatures\n",
    "    data = data.T  # Shape: (num_signatures, num_bins)\n",
    "    num_signatures, num_bins = data.shape\n",
    "\n",
    "    if n_clusters > num_signatures:\n",
    "        raise ValueError(\n",
    "            f\"n_clusters ({n_clusters}) is greater than total signatures ({num_signatures}).\"\n",
    "        )\n",
    "    \n",
    "    # Define the cosine distance metric\n",
    "    def custom_cosine_distance(vec_a, vec_b):\n",
    "        return cdist([vec_a], [vec_b], \"cosine\")[0, 0]\n",
    "\n",
    "    metric = distance_metric(type_metric.USER_DEFINED, func=custom_cosine_distance)\n",
    "\n",
    "    # Perform k-medoids clustering\n",
    "    # Pick random distinct indices as initial medoids\n",
    "    initial_medoids = random.sample(range(num_signatures), n_clusters)\n",
    "\n",
    "    # Convert data to a list of lists (required by pyclustering)\n",
    "    data_list = data.tolist()\n",
    "\n",
    "    # Run k-medoids (PAM)\n",
    "    kmedoids_instance = kmedoids(\n",
    "        data=data_list,\n",
    "        initial_index_medoids=initial_medoids,\n",
    "        metric=metric\n",
    "    )\n",
    "    kmedoids_instance.process()\n",
    "\n",
    "    # Retrieve clusters and medoids\n",
    "    clusters = kmedoids_instance.get_clusters()  # List of cluster indices\n",
    "    medoids = kmedoids_instance.get_medoids()    # Indices of medoids\n",
    "\n",
    "    # Retrieve the actual medoid signature vectors\n",
    "    medoid_signatures = np.array([data[idx] for idx in medoids])\n",
    "\n",
    "    return clusters, medoid_signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _______Signature matrices EXTRACTION from JSON_______\n",
    "\n",
    "results_dir = \"results\"   # Path to your JSON file's folder\n",
    "results_csv_dir = \"results_csv\" \n",
    "\n",
    "# Create the results_csv directory if it doesn't exist\n",
    "os.makedirs(results_csv_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the s_15, s_8, and s_25 folders exist\n",
    "for parent_folder in [\"s_15\", \"s_8\", \"s_25\"]:\n",
    "    parent_folder_path = os.path.join(results_csv_dir, parent_folder)\n",
    "    os.makedirs(parent_folder_path, exist_ok=True)\n",
    "\n",
    "    # Create noise level subfolders inside each parent folder\n",
    "    for noise_level in [\"n_0.02\", \"n_0.04\", \"n_0.08\"]:\n",
    "        noise_folder_path = os.path.join(parent_folder_path, noise_level)\n",
    "        os.makedirs(noise_folder_path, exist_ok=True)\n",
    "\n",
    "# Process each JSON file in the results directory\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "        \n",
    "        # Parse the JSON data\n",
    "        parsed_data = load_data(file_path)\n",
    "\n",
    "        # Extract the main folder name based on \"s_15\", \"s_8\", \"s_25\"\n",
    "        if \"s_15\" in filename:\n",
    "            parent_folder = \"s_15\"\n",
    "        elif \"s_8\" in filename:\n",
    "            parent_folder = \"s_8\"\n",
    "        elif \"s_25\" in filename:\n",
    "            parent_folder = \"s_25\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown parent folder for file: {filename}\")\n",
    "\n",
    "        # Extract the noise level from the filename\n",
    "        if \"n_0.02\" in filename:\n",
    "            noise_level = \"n_0.02\"\n",
    "        elif \"n_0.04\" in filename:\n",
    "            noise_level = \"n_0.04\"\n",
    "        elif \"n_0.08\" in filename:\n",
    "            noise_level = \"n_0.08\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown noise level for file: {filename}\")\n",
    "\n",
    "        # Create the parent folder and noise level folder\n",
    "        parent_folder_path = os.path.join(results_csv_dir, parent_folder)\n",
    "        noise_folder_path = os.path.join(parent_folder_path, noise_level)\n",
    "        os.makedirs(noise_folder_path, exist_ok=True)\n",
    "\n",
    "        # Get the subfolder name (without .json)\n",
    "        subfolder_name = os.path.splitext(filename)[0]\n",
    "        subfolder_name = subfolder_name[subfolder_name.find('_')+1:] if '_' in subfolder_name else subfolder_name\n",
    "        subfolder_path = os.path.join(noise_folder_path, subfolder_name)\n",
    "        \n",
    "        # Create the subfolder\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        # Create separate subfolders for Dw and Dkl\n",
    "        dw_folder_path = os.path.join(subfolder_path, \"Dw\")\n",
    "        dkl_folder_path = os.path.join(subfolder_path, \"Dkl\")\n",
    "        os.makedirs(dw_folder_path, exist_ok=True)\n",
    "        os.makedirs(dkl_folder_path, exist_ok=True)\n",
    "\n",
    "        # Save all Dw matrices to CSV\n",
    "        if 'all_Dw' in parsed_data:\n",
    "            for i, dw_matrix in parsed_data['all_Dw'].items():\n",
    "                dw_csv_path = os.path.join(dw_folder_path, f\"Dw_{i}.csv\")\n",
    "                dw_matrix.to_csv(dw_csv_path, index=False, header=False)  # Exclude index and header\n",
    "\n",
    "        # Save all Dkl matrices to CSV\n",
    "        if 'all_Dkl' in parsed_data:\n",
    "            for i, dkl_matrix in parsed_data['all_Dkl'].items():\n",
    "                dkl_csv_path = os.path.join(dkl_folder_path, f\"Dkl_{i}.csv\")\n",
    "                dkl_matrix.to_csv(dkl_csv_path, index=False, header=False)  # Exclude index and header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Signature Names: ['SBS10a', 'SBS56', 'SBS10d', 'SBS52', 'SBS36', 'SBS91', 'SBS45', 'SBS38']\n",
      "Filtered Signatures:\n",
      "                SBS10a     SBS56    SBS10d     SBS52     SBS36     SBS91  \\\n",
      "Type                                                                      \n",
      "A[C>A]A  2.190170e-03  0.012597  0.010114  0.015196  0.025194  0.002945   \n",
      "A[C>A]C  1.770137e-03  0.015697  0.018446  0.006538  0.008318  0.052997   \n",
      "A[C>A]G  1.500120e-04  0.000206  0.000727  0.004139  0.002239  0.000204   \n",
      "A[C>A]T  1.700132e-02  0.022995  0.014197  0.009238  0.017896  0.000131   \n",
      "A[C>G]A  2.230000e-16  0.000418  0.000129  0.001730  0.001840  0.000243   \n",
      "...               ...       ...       ...       ...       ...       ...   \n",
      "T[T>C]T  3.250252e-03  0.000285  0.007555  0.002000  0.002799  0.001274   \n",
      "T[T>G]A  2.690209e-03  0.009458  0.019898  0.001430  0.000788  0.005955   \n",
      "T[T>G]C  2.230000e-16  0.000001  0.000738  0.001120  0.000744  0.000143   \n",
      "T[T>G]G  2.160000e-05  0.000101  0.003148  0.001480  0.000902  0.000628   \n",
      "T[T>G]T  1.890147e-02  0.028994  0.014528  0.002529  0.001810  0.001009   \n",
      "\n",
      "            SBS45     SBS38  \n",
      "Type                         \n",
      "A[C>A]A  0.009108  0.012795  \n",
      "A[C>A]C  0.002849  0.010096  \n",
      "A[C>A]G  0.001660  0.001899  \n",
      "A[C>A]T  0.009638  0.008846  \n",
      "A[C>G]A  0.003109  0.002689  \n",
      "...           ...       ...  \n",
      "T[T>C]T  0.000545  0.003059  \n",
      "T[T>G]A  0.000285  0.001170  \n",
      "T[T>G]C  0.000135  0.000903  \n",
      "T[T>G]G  0.000708  0.001899  \n",
      "T[T>G]T  0.000724  0.005288  \n",
      "\n",
      "[96 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# _______COSMIC signatures exctraction_______\n",
    "# Define the filename\n",
    "filename = \"Results_20250116_194705_overall_s_8_n_0.02__GRCh37_10a_56_10d_52_36_91_45_38.json\"\n",
    "\n",
    "# Step 1: Extract signature names after \"GRCh37\"\n",
    "signature_portion = re.search(r\"GRCh37_([\\w\\d_]+)\", filename).group(1)  # Extracts \"10a_56_10d_52_36_91_45_38\"\n",
    "signature_ids = signature_portion.split(\"_\")  # Splits into ['10a', '56', '10d', '52', ...]\n",
    "\n",
    "# Prepend \"SBS\" to each signature ID\n",
    "signature_names = [f\"SBS{id}\" for id in signature_ids]\n",
    "print(\"Extracted Signature Names:\", signature_names)\n",
    "\n",
    "# Step 2: Load the COSMIC signatures file\n",
    "cosmic_signatures_path = \"cosmic_signatures/COSMIC_v3.4_SBS_GRCh37.txt\"\n",
    "signatures_df = pd.read_csv(cosmic_signatures_path, sep=\"\\t\")\n",
    "\n",
    "# Set the index if the \"Type\" column exists\n",
    "if \"Type\" in signatures_df.columns:\n",
    "    signatures_df.set_index(\"Type\", inplace=True)\n",
    "\n",
    "# Step 3: Filter the signatures corresponding to the extracted names\n",
    "filtered_signatures = signatures_df[signature_names]\n",
    "print(\"Filtered Signatures:\\n\", filtered_signatures)\n",
    "\n",
    "# Convert the filtered signatures to a NumPy array (if needed)\n",
    "true_signatures = filtered_signatures.to_numpy()\n",
    "true_sig_names = filtered_signatures.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
